
=== Deployment Considerations

==== High Availability

DSE is designed to remain constantly available, even in the face of hardware failures. In order to enable that capability, aspects of the cluster's physical deployment are used to determine how data replicas are placed to reduce the probability of simultaneous failures. In a conventional on-premise deployment, a DSE node's rack and data center are exposed. However in a cloud environment, it isn't typical to have access to the specific rack in which the node resides. Instead other mechanisms are required to infer physical separation of sets of nodes. In Azure, the available mechanisms are:

Cloud Service Fault Zones:: There are two Fault Zones per Cloud Service that map to physical isolation domains. These can potentially be used to map to DSE's rack concept. However, it is usally advisable for the number of racks to match or exceed the replication factor of the data. Given that the vast majority of DSE instances use a replication factor of three for most of the data, two Fault Zones in a Cloud Service isn't ideal. It is advisable to use Wide-VLAN to obtain more fault zones.

Upgrade Domains:: Azure maintains five upgrade domains that group servers together that will be unavailable when the Host OS is being serviced, upgraded or rebooted. These domains do not have a correlation with Fault Zones. DSE does not have a distinction between a physical failure and a node not being available due to an upgrade. These upgrade domains can also be mapped to the DSE rack-concept, but it is possible in that case that replica placement would happen entirely within one physical fault domain.

At this point, the recommendation is to *map Upgrade Domains to DSE racks*. Given the fact that if an actual physical failure occurs in a Fault Zone, the nodes will be immediately rebooted on other physical hosts, that type of event should result in only seconds to minutes of downtime while the nodes are booted to different physical servers. However, an outage in an upgrade domain could mean a significant amount of downtime for that set of replicas.
DataStax and Microsoft are collaborating on improving the situation further so that the Azure high availability infrastructure better maps to DSE's high availability mechanisms.

It is also recommended to use multi-region datacenters to further reduce the chance of loss of availability. That is described in the next section.


==== Storage Options

The scripts and instructions in this guide configure each node with 16 independent Azure disks spread across different storage accounts to optimize IO throughput. This is the highest performing general purpose configuration. However, it should be noted that read-heavy workloads could benefit from using *Host-Caching*. However, it is only possible to cache four Azure disks per instance with cached storage, so the overall throughput will be lower. However, the cache will provide significantly better performance for data in the cache, and will reduce the load on the backing storage. It is strongly recommended that ready-intensive workloads be tested on both the 16 disk configuration and with a four cached-disk configuration to determine which provides the best performance. More information on Host-Caching can be found in the http://msdn.microsoft.com/en-us/library/azure/dn790303.aspx[Azure Documentation]

Contrary to recommendations that DataStax provides for other clouds, using the local *Resource Disk* isn't recommended on Azure. Given the HA mapping considerations described above, it isn't possible to guarantee sufficient replica liveness to avoid data-loss with the use of temporary direct-attached storage.
